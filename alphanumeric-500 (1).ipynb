{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10705,"sourceType":"datasetVersion","datasetId":7160}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-29T20:11:49.287560Z","iopub.execute_input":"2025-09-29T20:11:49.287962Z","iopub.status.idle":"2025-09-29T20:11:51.692047Z","shell.execute_reply.started":"2025-09-29T20:11:49.287929Z","shell.execute_reply":"2025-09-29T20:11:51.691107Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/emnist/emnist-letters-mapping.txt\n/kaggle/input/emnist/emnist-letters-test.csv\n/kaggle/input/emnist/emnist-mnist-mapping.txt\n/kaggle/input/emnist/emnist-digits-train.csv\n/kaggle/input/emnist/emnist-bymerge-mapping.txt\n/kaggle/input/emnist/emnist-balanced-train.csv\n/kaggle/input/emnist/emnist-digits-test.csv\n/kaggle/input/emnist/emnist-balanced-test.csv\n/kaggle/input/emnist/emnist-mnist-test.csv\n/kaggle/input/emnist/emnist-letters-train.csv\n/kaggle/input/emnist/emnist-byclass-train.csv\n/kaggle/input/emnist/emnist-bymerge-test.csv\n/kaggle/input/emnist/emnist-balanced-mapping.txt\n/kaggle/input/emnist/emnist-mnist-train.csv\n/kaggle/input/emnist/emnist-digits-mapping.txt\n/kaggle/input/emnist/emnist-bymerge-train.csv\n/kaggle/input/emnist/emnist-byclass-test.csv\n/kaggle/input/emnist/emnist-byclass-mapping.txt\n/kaggle/input/emnist/emnist_source_files/emnist-digits-test-labels-idx1-ubyte\n/kaggle/input/emnist/emnist_source_files/emnist-bymerge-train-labels-idx1-ubyte\n/kaggle/input/emnist/emnist_source_files/emnist-letters-test-labels-idx1-ubyte\n/kaggle/input/emnist/emnist_source_files/emnist-byclass-train-images-idx3-ubyte\n/kaggle/input/emnist/emnist_source_files/emnist-byclass-test-images-idx3-ubyte\n/kaggle/input/emnist/emnist_source_files/emnist-mnist-train-images-idx3-ubyte\n/kaggle/input/emnist/emnist_source_files/emnist-digits-train-labels-idx1-ubyte\n/kaggle/input/emnist/emnist_source_files/emnist-bymerge-test-images-idx3-ubyte\n/kaggle/input/emnist/emnist_source_files/emnist-bymerge-test-labels-idx1-ubyte\n/kaggle/input/emnist/emnist_source_files/emnist-mnist-test-labels-idx1-ubyte\n/kaggle/input/emnist/emnist_source_files/emnist-balanced-test-labels-idx1-ubyte\n/kaggle/input/emnist/emnist_source_files/emnist-balanced-test-images-idx3-ubyte\n/kaggle/input/emnist/emnist_source_files/emnist-mnist-test-images-idx3-ubyte\n/kaggle/input/emnist/emnist_source_files/emnist-bymerge-train-images-idx3-ubyte\n/kaggle/input/emnist/emnist_source_files/emnist-letters-train-labels-idx1-ubyte\n/kaggle/input/emnist/emnist_source_files/emnist-digits-train-images-idx3-ubyte\n/kaggle/input/emnist/emnist_source_files/emnist-byclass-train-labels-idx1-ubyte\n/kaggle/input/emnist/emnist_source_files/emnist-balanced-train-images-idx3-ubyte\n/kaggle/input/emnist/emnist_source_files/emnist-balanced-train-labels-idx1-ubyte\n/kaggle/input/emnist/emnist_source_files/emnist-letters-test-images-idx3-ubyte\n/kaggle/input/emnist/emnist_source_files/emnist-byclass-test-labels-idx1-ubyte\n/kaggle/input/emnist/emnist_source_files/emnist-letters-train-images-idx3-ubyte\n/kaggle/input/emnist/emnist_source_files/emnist-mnist-train-labels-idx1-ubyte\n/kaggle/input/emnist/emnist_source_files/emnist-digits-test-images-idx3-ubyte\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"crawford/emnist\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T20:13:18.417759Z","iopub.execute_input":"2025-09-29T20:13:18.418069Z","iopub.status.idle":"2025-09-29T20:13:18.876115Z","shell.execute_reply.started":"2025-09-29T20:13:18.418045Z","shell.execute_reply":"2025-09-29T20:13:18.874702Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/emnist\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Import necessary libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\nfrom scipy.stats import pearsonr\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Check TensorFlow version and GPU availability\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\nprint(f\"Keras version: {keras.__version__}\")\n\n# Set random seeds for reproducibility\ntf.random.set_seed(42)\nnp.random.seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T20:13:28.946795Z","iopub.execute_input":"2025-09-29T20:13:28.947185Z","iopub.status.idle":"2025-09-29T20:13:34.527883Z","shell.execute_reply.started":"2025-09-29T20:13:28.947149Z","shell.execute_reply":"2025-09-29T20:13:34.525894Z"}},"outputs":[{"name":"stdout","text":"TensorFlow version: 2.18.0\nGPU Available: []\nKeras version: 3.8.0\n","output_type":"stream"},{"name":"stderr","text":"2025-09-29 20:13:34.520331: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Load EMNIST dataset from local path\nimport os\nimport pandas as pd\nimport numpy as np\n\nprint(\"Loading EMNIST dataset from local path...\")\n\n# Path to the EMNIST dataset\ndataset_path = path\n\ntry:\n    # List files in the dataset directory\n    files = os.listdir(dataset_path)\n    print(f\"Files in dataset directory: {files}\")\n    \n    # Look for EMNIST ByClass files (62 classes: 0-9, A-Z, a-z)\n    train_file = None\n    test_file = None\n    \n    for file in files:\n        if 'byclass' in file.lower() and 'train' in file.lower():\n            train_file = file\n        elif 'byclass' in file.lower() and 'test' in file.lower():\n            test_file = file\n    \n    if train_file and test_file:\n        print(f\"Found training file: {train_file}\")\n        print(f\"Found test file: {test_file}\")\n        \n        # Load the CSV files\n        train_data = pd.read_csv(os.path.join(dataset_path, train_file))\n        # Filter to include only classes 0-35 (digits 0-9 and uppercase A-Z)\n        train_data = train_data[train_data.iloc[:, 0].isin(np.arange(0, 36))]\n        test_data = pd.read_csv(os.path.join(dataset_path, test_file))\n        test_data = test_data[test_data.iloc[:, 0].isin(np.arange(0, 36))]\n        \n        print(f\"Training data shape: {train_data.shape}\")\n        print(f\"Test data shape: {test_data.shape}\")\n        \n        # Extract features and labels\n        # First column is label, rest are pixel values\n        x_train = train_data.iloc[:, 1:].values\n        y_train = train_data.iloc[:, 0].values\n        x_test = test_data.iloc[:, 1:].values\n        y_test = test_data.iloc[:, 0].values\n        \n        # Reshape from flat arrays to 28x28 images\n        x_train = x_train.reshape(-1, 28, 28)\n        x_test = x_test.reshape(-1, 28, 28)\n        \n        # EMNIST images need to be rotated and flipped to match standard orientation\n        x_train = np.rot90(x_train, k=3, axes=(1, 2))  # Rotate 270 degrees\n        x_test = np.rot90(x_test, k=3, axes=(1, 2))    # Rotate 270 degrees\n        x_train = np.flip(x_train, axis=2)  # Flip horizontally\n        x_test = np.flip(x_test, axis=2)    # Flip horizontally\n        \n        num_classes = len(np.unique(y_train))\n        \n        print(f\"\\nEMNIST ByClass Dataset loaded successfully!\")\n        print(f\"Training data shape: {x_train.shape}\")\n        print(f\"Training labels shape: {y_train.shape}\")\n        print(f\"Test data shape: {x_test.shape}\")\n        print(f\"Test labels shape: {y_test.shape}\")\n        print(f\"Number of classes: {num_classes}\")\n        print(f\"Label range: {np.min(y_train)} to {np.max(y_train)}\")\n        \n        # Create class mapping\n        class_names = []\n        # Classes 0-9: Digits\n        class_names.extend([str(i) for i in range(10)])\n        # Classes 10-35: Uppercase letters A-Z\n        class_names.extend([chr(ord('A') + i) for i in range(26)])\n        \n        print(f\"Class mapping: 0-9 (digits), 10-35 (A-Z)\")\n        print(f\"Sample classes: {class_names[:10]}...{class_names[-10:]}\")\n        \n    else:\n        raise FileNotFoundError(\"Could not find ByClass train/test files\")\n\nexcept Exception as e:\n    print(f\"Error loading dataset: {e}\")\n    raise e","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T20:13:39.397804Z","iopub.execute_input":"2025-09-29T20:13:39.398447Z","iopub.status.idle":"2025-09-29T20:15:00.642495Z","shell.execute_reply.started":"2025-09-29T20:13:39.398417Z","shell.execute_reply":"2025-09-29T20:15:00.641055Z"}},"outputs":[{"name":"stdout","text":"Loading EMNIST dataset from local path...\nFiles in dataset directory: ['emnist-letters-mapping.txt', 'emnist-letters-test.csv', 'emnist-mnist-mapping.txt', 'emnist-digits-train.csv', 'emnist-bymerge-mapping.txt', 'emnist-balanced-train.csv', 'emnist-digits-test.csv', 'emnist-balanced-test.csv', 'emnist-mnist-test.csv', 'emnist-letters-train.csv', 'emnist-byclass-train.csv', 'emnist-bymerge-test.csv', 'emnist-balanced-mapping.txt', 'emnist-mnist-train.csv', 'emnist-digits-mapping.txt', 'emnist-bymerge-train.csv', 'emnist-byclass-test.csv', 'emnist_source_files', 'emnist-byclass-mapping.txt']\nFound training file: emnist-byclass-train.csv\nFound test file: emnist-byclass-test.csv\nTraining data shape: (533992, 785)\nTest data shape: (89263, 785)\n\nEMNIST ByClass Dataset loaded successfully!\nTraining data shape: (533992, 28, 28)\nTraining labels shape: (533992,)\nTest data shape: (89263, 28, 28)\nTest labels shape: (89263,)\nNumber of classes: 36\nLabel range: 0 to 35\nClass mapping: 0-9 (digits), 10-35 (A-Z)\nSample classes: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']...['Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Image Preprocessing\nprint(\"Original data shapes:\")\nprint(f\"x_train: {x_train.shape}, x_test: {x_test.shape}\")\nprint(f\"y_train: {y_train.shape}, y_test: {y_test.shape}\")\nprint(f\"Pixel value range: {x_train.min()} to {x_train.max()}\")\n\n# 1. Normalization: Scale pixel values to [0, 1] range\n# Formula: I_norm = I/255\nx_train_norm = x_train.astype('float32') / 255.0\nx_test_norm = x_test.astype('float32') / 255.0\n\nprint(f\"\\nAfter normalization:\")\nprint(f\"Pixel value range: {x_train_norm.min()} to {x_train_norm.max()}\")\n\n# 2. Reshaping for CNN input: Add channel dimension\n# From (28, 28) to (28, 28, 1)\nx_train_reshaped = x_train_norm.reshape(x_train_norm.shape[0], 28, 28, 1)\nx_test_reshaped = x_test_norm.reshape(x_test_norm.shape[0], 28, 28, 1)\n\nprint(f\"\\nAfter reshaping:\")\nprint(f\"x_train: {x_train_reshaped.shape}, x_test: {x_test_reshaped.shape}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-29T19:53:18.748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Label encoding: Convert to one-hot encoded vectors\ny_train_onehot = keras.utils.to_categorical(y_train, num_classes)\ny_test_onehot = keras.utils.to_categorical(y_test, num_classes)\n\nprint(f\"\\nAfter one-hot encoding:\")\nprint(f\"y_train: {y_train_onehot.shape}, y_test: {y_test_onehot.shape}\")\nprint(f\"Example label before: {y_train[0]}\")\nprint(f\"Example label after: {y_train_onehot[0]}\")\n\n# Store preprocessed data\nx_train_processed = x_train_reshaped\nx_test_processed = x_test_reshaped\ny_train_processed = y_train_onehot\ny_test_processed = y_test_onehot","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-29T19:53:18.748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize sample images from the dataset\nplt.figure(figsize=(12, 8))\nfor i in range(20):\n    plt.subplot(4, 5, i + 1)\n    plt.imshow(x_train[i], cmap='gray')\n    plt.title(f'Label: {y_train[i]}')\n    plt.axis('off')\n\nplt.suptitle('Sample Images from MNIST Dataset', fontsize=16)\nplt.tight_layout()\nplt.show()\n\n# Show distribution of classes\nplt.figure(figsize=(10, 6))\nunique, counts = np.unique(y_train, return_counts=True)\nplt.bar(unique, counts)\nplt.title('Distribution of Classes in Training Set')\nplt.xlabel('Digit Class')\nplt.ylabel('Number of Samples')\nplt.xticks(unique)\nplt.grid(True, alpha=0.3)\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-29T19:53:18.749Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define CNN Architecture for Extended Alphanumeric Dataset\ndef create_cnn_model(input_shape=(28, 28, 1), num_classes=62):\n    \"\"\"\n    Create CNN model according to requirements specifications\n    Updated for 62 classes (0-9, A-Z, a-z)\n    \"\"\"\n    model = keras.Sequential([\n        # Input Layer\n        keras.Input(shape=input_shape),\n        \n        # Layer 1: Conv2D with 32 filters, 3x3 kernel, ReLU activation\n        layers.Conv2D(32, (3, 3), activation='relu', padding='valid', name='conv2d_1'),\n        \n        # Layer 2: MaxPooling2D with 2x2 pool size\n        layers.MaxPooling2D((2, 2), name='maxpool2d_1'),\n        \n        # Layer 3: Conv2D with 64 filters, 3x3 kernel, ReLU activation\n        layers.Conv2D(64, (3, 3), activation='relu', padding='valid', name='conv2d_2'),\n        \n        # Layer 4: MaxPooling2D with 2x2 pool size\n        layers.MaxPooling2D((2, 2), name='maxpool2d_2'),\n        \n        # Layer 5: Flatten layer (feature extractor)\n        layers.Flatten(name='flatten'),\n        \n        # Layer 6: Dense layer with 128 units, ReLU activation\n        layers.Dense(128, activation='relu', name='dense_1'),\n        \n        # Layer 7: Output Dense layer with 62 classes for alphanumeric characters\n        layers.Dense(num_classes, activation='softmax', name='output')\n    ])\n    \n    return model\n\n# Create the model with 62 classes\ncnn_model = create_cnn_model(input_shape=(28, 28, 1), num_classes=num_classes)\n\n# Display model architecture\nprint(\"CNN Model Architecture for Extended Alphanumeric Dataset:\")\nprint(f\"Number of classes: {num_classes}\")\ncnn_model.summary()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-29T19:53:18.751Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show the class mapping\nprint(f\"\\nClass Mapping:\")\nprint(f\"0-9: Digits\")\nprint(f\"10-35: Uppercase letters (A-Z)\")\nprint(f\"Total classes: {num_classes}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-29T19:53:18.751Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compile the CNN model\ncnn_model.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nprint(\"Model compiled successfully!\")\nprint(\"Optimizer: Adam\")\nprint(\"Loss Function: Categorical Cross-Entropy\")\nprint(\"Metrics: Accuracy\")\n\n\n# Train the model\nprint(\"\\nStarting CNN training...\")\nhistory = cnn_model.fit(\n    x_train_processed, y_train_processed,\n    batch_size=16,\n    epochs=20,\n    validation_split=0.2,\n    verbose=1\n)\n\nprint(\"\\nCNN training completed!\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-29T19:53:18.752Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot training history\nplt.figure(figsize=(15, 5))\n\n# Plot training & validation accuracy\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Plot training & validation loss\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Print final training results\nprint(\"Final Training Results:\")\nprint(f\"Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\nprint(f\"Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\nprint(f\"Training Loss: {history.history['loss'][-1]:.4f}\")\nprint(f\"Validation Loss: {history.history['val_loss'][-1]:.4f}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-29T19:53:18.753Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate Standalone CNN Performance\nprint(\"Evaluating CNN on Test Set...\")\nprint(\"=\" * 50)\n\n# Get CNN predictions on test set\ncnn_predictions = cnn_model.predict(x_test_processed, batch_size=32, verbose=1)\ncnn_predicted_classes = np.argmax(cnn_predictions, axis=1)\n\n# Calculate CNN metrics\ncnn_correct_predictions = np.sum(cnn_predicted_classes == y_test)\ncnn_accuracy = cnn_correct_predictions / len(y_test)\ncnn_error_rate = 1 - cnn_accuracy\n\nprint(f\"\\nStandalone CNN Results:\")\nprint(f\"Correct predictions: {cnn_correct_predictions}/{len(y_test)}\")\nprint(f\"Accuracy: {cnn_accuracy:.4f} ({cnn_accuracy*100:.2f}%)\")\nprint(f\"Error Rate: {cnn_error_rate:.4f} ({cnn_error_rate*100:.2f}%)\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-29T19:53:18.754Z"}},"outputs":[],"execution_count":null}]}